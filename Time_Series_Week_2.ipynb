{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time Series Week 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTJx8Lb20oXGBnWbXRiLet",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denmau/tensorflow-practice/blob/main/Time_Series_Week_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMveWvNx7Ha-",
        "outputId": "180a74c0-8af9-4501-b502-b7df7c458026"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jai6SgWESOo",
        "outputId": "78c2ff93-d306-42f1-bd38-98171a1935cd"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "for val in dataset:\n",
        "  print(val.numpy())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpy1JcdOEjFr",
        "outputId": "ce7d568a-5cde-42ad-f325-c42afd6aa14c"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1)\n",
        "# drop_remainder is False by default\n",
        "for window_dataset in dataset:\n",
        "  for val in window_dataset:\n",
        "    print(val.numpy(), end=\" \")\n",
        "  print()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 2 3 4 \n",
            "1 2 3 4 5 \n",
            "2 3 4 5 6 \n",
            "3 4 5 6 7 \n",
            "4 5 6 7 8 \n",
            "5 6 7 8 9 \n",
            "6 7 8 9 \n",
            "7 8 9 \n",
            "8 9 \n",
            "9 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYgnHgX17X46",
        "outputId": "d1e90324-022a-4834-c7d8-b79d34c7225c"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
        "for window_dataset in dataset:\n",
        "  for val in window_dataset:\n",
        "    print(val.numpy(), end=\" \")\n",
        "  print()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 2 3 4 \n",
            "1 2 3 4 5 \n",
            "2 3 4 5 6 \n",
            "3 4 5 6 7 \n",
            "4 5 6 7 8 \n",
            "5 6 7 8 9 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQdmo_l07-6Z",
        "outputId": "c584e5c4-ec3f-4971-935f-9670392272ba"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5)) # flatten the dataset\n",
        "for window in dataset:\n",
        "  print(window.numpy())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4]\n",
            "[1 2 3 4 5]\n",
            "[2 3 4 5 6]\n",
            "[3 4 5 6 7]\n",
            "[4 5 6 7 8]\n",
            "[5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WEpIuxsAWXX"
      },
      "source": [
        "Split the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMoSbpZEAFaA",
        "outputId": "bd999d19-616a-445d-a052-24776614eec1"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5)) # flatten the dataset\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1:])) \n",
        "# Splitting into everything but the last one(:-1), and the last one only(-1:)\n",
        "for x,y in dataset:\n",
        "  print(x.numpy(), y.numpy())"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3] [4]\n",
            "[1 2 3 4] [5]\n",
            "[2 3 4 5] [6]\n",
            "[3 4 5 6] [7]\n",
            "[4 5 6 7] [8]\n",
            "[5 6 7 8] [9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzoQmvshCEhl"
      },
      "source": [
        "shuffle data before training to prevent sequence biass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DycfBN5CGM9",
        "outputId": "911b2c50-9315-48a0-c179-3072408f80b0"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5)) # flatten the dataset\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1])) \n",
        "# Splitting into everything but the last one(:-1)-input x, and the last one itself(-1:)-label y\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size=10) \n",
        "# buffer size is the number of data items that we have\n",
        "\n",
        "for x,y in dataset:\n",
        "  print(x.numpy(), y.numpy())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3] 4\n",
            "[2 3 4 5] 6\n",
            "[3 4 5 6] 7\n",
            "[5 6 7 8] 9\n",
            "[1 2 3 4] 5\n",
            "[4 5 6 7] 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH3mwoEgD9DR"
      },
      "source": [
        "Batch the data (into sets of 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncRArIx-ClbM",
        "outputId": "3e33fc61-5011-4beb-ddd1-f1b71f89b1ed"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5)) # flatten the dataset\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1])) \n",
        "# Splitting into everything but the last one(:-1), and the last one itself(-1:)\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size=10) \n",
        "# buffer size is the number of data items that we have\n",
        "\n",
        "dataset = dataset.batch(2).prefetch(1)\n",
        "\n",
        "for x,y in dataset:\n",
        "  print('x = ', x.numpy())\n",
        "  print('y = ', y.numpy())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x =  [[4 5 6 7]\n",
            " [5 6 7 8]]\n",
            "y =  [8 9]\n",
            "x =  [[1 2 3 4]\n",
            " [2 3 4 5]]\n",
            "y =  [5 6]\n",
            "x =  [[0 1 2 3]\n",
            " [3 4 5 6]]\n",
            "y =  [4 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UdRH4-yEBdl"
      },
      "source": [
        "# Single layer Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8eq5aItECa8"
      },
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "  # we will pass the series through it from the tensor_slices(series)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "  # slice the data into appropriate windows each one being shifted by 1 an keep all in same size\n",
        "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "  # flatten data into chunks of size window_size + 1\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "  # now shuffle the flattened data. Shuffle buffer tends to speed things up even in super large datasets\n",
        "  dataset = dataset.shuffle(shuffle_buffer)\n",
        "  # shuffled data is then splitted into x's which is all elements except the last, and y which is the last element\n",
        "  dataset = dataset.map(lambda window: (window[:-1], window[-1])) \n",
        "  # or we can combine this with shuffle\n",
        "  #     dataset = dataset.shuffle(shuffle_buffer)\n",
        "  # it is then batched into the selected batch_size\n",
        "  dataset = dataset.batch(batch_size).prefetch(1)\n",
        "  return dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DMBGTFraxOe"
      },
      "source": [
        "# split dataset into training & Validation set\n",
        "split_time = 1000 # splitting at time stamp 1000\n",
        "\n",
        "# training data is the subset of the series upto split_time\n",
        "time_train = time[:split_time] \n",
        "x_train = series[:split_time]\n",
        "\n",
        "time_valid = time[:split_time]\n",
        "x_valid = series[:split_time]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACa2UB-Tca9_"
      },
      "source": [
        "## our Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G2OTZ_qcbim"
      },
      "source": [
        "# define the series\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHgWO2dCb72D"
      },
      "source": [
        "# Linear Regression\n",
        "window_size = 20\n",
        "batch_size = 32\n",
        "shuffle_buffle_size = 1000\n",
        "\n",
        "#lets create our dataset\n",
        "dataset = windowed_dataset(series, window_size, batch_size, shuffle_buffer_size)\n",
        "# single dense layer\n",
        "l0 = tf.keras.layers.Dense(1, input_shape = [window_size])\n",
        "\n",
        "# Model\n",
        "model = tf.keras.models.Sequential(l0)\n",
        "\n",
        "# compile model\n",
        "#   mean squared error loss function\n",
        "#   optimizer will Stochastic Gradient Descent\n",
        "model.compile(loss=\"mse\", optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
        "model.fit(dataset, epochs=100, verbose=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvy0Tem-d1i2"
      },
      "source": [
        "# inspect the different weights\n",
        "print(f\"Layer weights {l0.get_weights()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9lXDWw7fS78"
      },
      "source": [
        "# print 20 items in series\n",
        "print(series[1:21])\n",
        "# predict the series\n",
        "# the numpy new axis reshapes it to input dimension useed by the model\n",
        "model.predict(series[1:21])\n",
        "\n",
        "# our model says, when it sees 20 values like this, the predicted value is ____________"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXk_lndOgLtk"
      },
      "source": [
        "# plot our forecast for every point on the timeseries, relative to the time series before it (our window size is 20)\n",
        "forecast = []\n",
        "# iterate over the series \n",
        "for time in range(len(series) - window_size):\n",
        "  # iterate over the series taking sices in window size, predicting them the append the prediction in forecast\n",
        "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
        "\n",
        "# take the forecast after the split time and load into np array for charting\n",
        "forecast = forecast[split_time - window_size:]\n",
        "results = np.array(forecast)[:, 0, 0]\n",
        "\n",
        "# chart results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se1hpaw3hq5I"
      },
      "source": [
        "# measure mean abs error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwNiQIhsivHo"
      },
      "source": [
        "# Using a DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV7ddVCHix7r"
      },
      "source": [
        " # the difference is of course the model\n",
        " dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        " \n",
        " model = tf.keras.models.Sequential([\n",
        "                                     tf.keras.layers.Dense(10, input_shape = [window_size], activation=\"relu\"),\n",
        "                                     tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "                                     tf.keras.layers.Dense(1) # this single neuron dense gives us the predicted value\n",
        " ])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
        "model.fit(dataset, epochs=100, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JkAIg_LkCIy"
      },
      "source": [
        "# DNN with Callbacks\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        " \n",
        "model = tf.keras.models.Sequential([\n",
        "                                     tf.keras.layers.Dense(10, input_shape = [window_size], activation=\"relu\"),\n",
        "                                     tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "                                     tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# The Callback changes the learning rate to a value based on the epoch number\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer = optimizer)\n",
        "model.fit(dataset, epochs=100, callbacks=[lr_schedule])\n",
        "\n",
        "# chart loss per epoch against the learningrate per epoch\n",
        "# we can then pick the lost point of chart where is still relatively stable, then update it to the learning rate"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}